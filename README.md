# rllm-toxicity-evaluation
Современные языковые модели (LLM) показывают высокие результаты в обработке естественного языка. Влияние языковых моделей значительно возросло за последние годы, эта технология используется для генерации текста, выявления контекста, в качестве помощника в поиске информации и других целей. Однако модели могут содержать встроенные предвзятости, проявляющиеся в дискриминации, уничижении или стереотипизации. Одним из наиболее актуальных видов смещенности является токсичность, которая проявляется в виде оскорбительных, уничижительных или ненавистных высказываний. Токсичность в русскоязычных моделях может быть особенно опасной, так как она не только отражает существующие предвзятости в обществе, но и может усиливать их, распространяя негативные стереотипы и дискриминацию. Здесь представлен программный комплекс, позволяющий выявить и исследовать смещенность больших языковых моделей русского языка в аспекте токсичности.
